# üß™ EVIDENCE SYSTEM - VALIDATION CHECKLIST

## ‚ö° Performance Testing (CR√çTICO)

### Acceptance Gate
**Este benchmarking es OBLIGATORIO antes de considerar el sistema production-ready.**

No es opcional. Si no se puede correr ahora, al menos el "c√≥mo" debe estar ultra concreto.

---

### Test 1: Baseline sin Evidence System

**Setup:**
```typescript
// En EvidenceAIGenerator.tsx, agregar flag temporal:
const EVIDENCE_DISABLED = true; // ‚Üê Para medir baseline

if (EVIDENCE_DISABLED) {
  // Skip evidence modal, call function directly
  const result = await supabase.functions.invoke(functionName, { body });
  return result;
}
```

**Protocolo de medici√≥n:**

1. **Instrumentar con timestamps:**
   ```typescript
   const startTime = performance.now();
   const result = await supabase.functions.invoke(...);
   const endTime = performance.now();
   const duration = endTime - startTime;

   // Log to analytics
   await logMetric({
     feature: 'AITaskGenerator',
     mode: 'baseline',
     duration_ms: duration,
     timestamp: new Date().toISOString()
   });
   ```

2. **Ejecutar N veces por feature:**
   - [ ] AITaskGenerator - 20 ejecuciones
   - [ ] AILeadFinder - 20 ejecuciones
   - [ ] LearningPathGenerator - 20 ejecuciones
   - [ ] OneOnOnePrep - 20 ejecuciones
   - [ ] GeoIntelligenceSelector - 20 ejecuciones

3. **Calcular percentiles:**
   ```typescript
   const durations = [/* array of 20 measurements */];
   durations.sort((a, b) => a - b);

   const p50 = durations[Math.floor(durations.length * 0.5)];
   const p95 = durations[Math.floor(durations.length * 0.95)];

   console.log({ feature, p50, p95 });
   ```

**Output esperado:**
```json
{
  "AITaskGenerator": { "p50": 2.1, "p95": 2.8 },
  "AILeadFinder": { "p50": 3.4, "p95": 4.2 },
  // ...
}
```

### Test 2: Con Evidence Mode = Balanced

**M√©tricas a medir:** p50 (mediana) + p95 (worst case aceptable)

**L√≠mites absolutos por tipo de feature:**

| Feature Type | p50 Target | p95 Max | Rationale |
|--------------|-----------|---------|-----------|
| **Tasks** (AITaskGenerator, Router, Executor) | < 3s | < 5s | Decisiones internas, retrieval m√≠nimo |
| **CRM** (AILeadFinder) | < 5s | < 7s | B√∫squeda r√°pida, no bloquear prospecting |
| **Learning** (LearningPathGenerator) | < 8s | < 12s | Contenido m√°s profundo, usuario espera |
| **Team** (OneOnOnePrep, Scheduling) | < 6s | < 10s | Preparaci√≥n de meeting, no urgente |
| **GeoIntelligence** | < 7s | < 12s | APIs externas, pero cr√≠tico para onboarding |

**Acceptance criteria:**
- ‚úÖ Cumple p50 + p95 ‚Üí OK
- ‚ö†Ô∏è Cumple p50 pero falla p95 ‚Üí Hay outliers, investigar
- ‚ùå Falla p50 ‚Üí **BLOCKER** - Reducir scope inmediatamente

**Importante:** Un incremento del 30% puede ser OK (2s‚Üí2.6s) o fatal (12s‚Üí16s). Por eso medimos l√≠mites absolutos.

### Test 3: Con Evidence Mode = Strict

**Solo para features donde tiene sentido:** financial, CRM, learning

**L√≠mites absolutos:**

| Feature Type | p50 Target | p95 Max |
|--------------|-----------|---------|
| **Financial (strict)** | < 15s | < 30s |
| **CRM (strict)** | < 10s | < 20s |
| **Learning (strict)** | < 12s | < 25s |

**Acceptance criteria:**
- ‚úÖ Cumple l√≠mites ‚Üí OK (el usuario pidi√≥ strict, acepta esperar)
- ‚ùå p95 > 30s ‚Üí Timeout probable - Reducir tiers o implementar early-exit

---

## üéØ Coverage Testing

### Por Profile:

#### Tasks Profile (ULTRA-LIVIANO + CLAIM-BASED RETRIEVAL)

**Mecanismo de activaci√≥n de retrieval:**

La clave es que **retrieval solo se activa si el output va a incluir n√∫meros externos**.

**Implementaci√≥n:**

1. **En FUNCTION_CLAIMS para tasks:**
   ```typescript
   // La mayor√≠a de claims NO requieren evidencia
   {
     claim: "Tareas generadas basadas en contexto del proyecto",
     requires_evidence: false,  // ‚Üê DEFAULT
     minSources: 0
   }

   // Solo si usuario pide comparaci√≥n con industria:
   {
     claim: "Tiempo estimado comparado con benchmark de industria",
     requires_evidence: true,  // ‚Üê Solo si user pidi√≥ "comparado con..."
     minSources: 2
   }
   ```

2. **Detecci√≥n autom√°tica en prompt:**
   ```typescript
   // Si user input contiene keywords:
   const needsExternalData = /comparado|benchmark|industria|promedio|est√°ndar/.test(userInput);

   if (needsExternalData) {
     // Activar claim con requires_evidence: true
   } else {
     // retrieval = 0 (solo contexto interno)
   }
   ```

**Test cases:**

- [ ] **Generaci√≥n normal**: "Genera tareas para esta semana"
  - Expected: retrieval = 0, sources = 0, output basado en internal_data ‚úÖ

- [ ] **Con benchmark externo**: "Genera tareas con tiempo promedio de la industria"
  - Expected: retrieval activado, busca en official_apis, cita benchmarks ‚úÖ

- [ ] **Modo strict manual**: Usuario cambia a strict en modal
  - Expected: retrieval forzado aunque no haya claim factual (user override) ‚úÖ

**Expected behavior:**
```typescript
// Normal (hypothesis) ‚Üí retrieval=0, sources=0 ‚Üí OK
// Con benchmark (balanced) ‚Üí retrieval=1, sources=2+ ‚Üí OK
// Strict manual (strict) ‚Üí retrieval=1, sources=2+ ‚Üí Required
```

#### Financial Profile

**GeoIntelligence - Graceful Degradation Contract:**

Testear con 2 ciudades grandes + 3 peque√±as:
- [ ] Madrid (grande) ‚Üí ¬øEncuentra datos en official_apis?
- [ ] Barcelona (grande) ‚Üí ¬øEncuentra datos en official_apis?
- [ ] Cuenca (peque√±a) ‚Üí ¬øDegrada sin romper?
- [ ] Teruel (peque√±a) ‚Üí ¬øDegrada sin romper?
- [ ] Soria (peque√±a) ‚Üí ¬øDegrada sin romper?

**Contrato de comportamiento:**

```typescript
// SIEMPRE debe devolver un resultado √∫til
interface GeoIntelligenceResult {
  success: true,
  data: {
    city: string,
    country: string,
    evidence_status: 'verified' | 'partial' | 'no_evidence',
    sources_found: number,
    // ... rest of data (SIEMPRE presente, aunque sea qualitative)
  }
}

// NUNCA debe romper con error si no hay fuentes
```

**Expected behavior por escenario:**

1. **Ciudad grande + APIs disponibles:**
   ```typescript
   evidence_status = 'verified'
   sources_found >= minSourcesOverall
   UI: Muestra datos con badge "‚úì Verificado"
   ```

2. **Ciudad peque√±a + APIs sin datos:**
   ```typescript
   evidence_status = 'no_evidence'
   sources_found = 0
   UI: Muestra datos con badge "‚ö†Ô∏è Hip√≥tesis" + bot√≥n "üîç Buscar m√°s evidencia"
   Output: DEBE ser cualitativo pero √∫til (no romper)
   ```

3. **APIs down / timeout:**
   ```typescript
   evidence_status = 'partial'
   sources_found < minSourcesOverall
   UI: Muestra datos con badge "‚ö†Ô∏è Datos limitados" + retry button
   ```

**Cr√≠tico:** El sistema NUNCA debe fallar silenciosamente ni devolver null. Siempre generar output √∫til.

#### CRM Profile
- [ ] **AILeadFinder**: ¬øEncuentra competidores reales?
- [ ] **AILeadFinder**: ¬øDatos de LinkedIn/Crunchbase accesibles?

#### Learning Profile
- [ ] **LearningPathGenerator**: ¬øEncuentra cursos reales?
- [ ] **LearningPathGenerator**: ¬øAPIs educativas (Coursera, Udemy) disponibles?

---

## üîç Retrieval Strategy Validation

### Test por Tier:

#### Tier 1: user_docs
- [ ] ¬øExtracci√≥n de PDF/CSV funciona?
- [ ] ¬øTags manuales se aplican correctamente?
- [ ] ¬øB√∫squeda sem√°ntica sobre docs?

#### Tier 2: official_apis
- [ ] ¬øQu√© APIs est√°n realmente integradas?
- [ ] ¬øRate limits manejados?
- [ ] ¬øFallback a web_news si API falla?

#### Tier 3: internal_data
- [ ] ¬øConsultas a DB optimizadas?
- [ ] ¬ø√çndices en tablas relevantes?

#### Tier 4: web_news
- [ ] ¬øWeb scraping permitido legalmente?
- [ ] ¬øFuentes confiables definidas?

---

## üö® Red Flags a Vigilar

### 1. Retrieval Waste (M√âTRICA CR√çTICA)

**Definici√≥n:** Cuando el sistema busca evidencias pero luego no las usa.

**F√≥rmula:**
```typescript
waste_rate = (sources_found > 0 && coverage_percentage < threshold)
  ? 1
  : 0

// Aggregate:
waste_rate_total = sum(waste) / total_generations
```

**Thresholds por modo:**
- **Hypothesis:** No aplica (no busca)
- **Balanced:** threshold = 30% (si encuentra fuentes, al menos 30% deben citarse)
- **Strict:** threshold = 50% (si encuentra fuentes, al menos 50% deben citarse)

**Red flags:**

| Scenario | sources_found | coverage_% | Waste? | Acci√≥n |
|----------|---------------|------------|--------|--------|
| B√∫squeda in√∫til | 5 | 0% | ‚úÖ YES | Ajustar queryHints, prompts de citaci√≥n |
| B√∫squeda parcial | 3 | 20% | ‚úÖ YES | Mejorar relevancia de sources |
| B√∫squeda OK | 4 | 60% | ‚ùå NO | Todo bien |
| Sin b√∫squeda | 0 | 0% | ‚ùå NO | Hypothesis mode, esperado |

**Causas t√≠picas de waste:**
1. **Profile mal asignado:** Busca en tiers irrelevantes
2. **Keywords gen√©ricos:** Encuentra sources pero no relevantes
3. **Prompts no citan:** LLM genera sin usar las sources
4. **Claims desalineados:** Se busca para claims que no se usan

**Fix por causa:**
1. Revisar tierOrder y profile assignment
2. Agregar synonyms espec√≠ficos al domain
3. Mejorar prompt para forzar citaci√≥n
4. Alinear FUNCTION_CLAIMS con output real

**Target metric:**
- ‚úÖ waste_rate < 20% ‚Üí Sistema eficiente
- ‚ö†Ô∏è waste_rate 20-40% ‚Üí Revisar prompts y claims
- ‚ùå waste_rate > 40% ‚Üí **PROBLEMA SERIO** - Retrieval in√∫til

### 2. Timeout en Generaci√≥n
**S√≠ntoma:** Edge function tarda >30s

**Causas:**
- Demasiados tiers en parallel
- APIs externas lentas
- Sin timeout en retrieval

**Fix:**
- Reducir tierOrder (eliminar web_news si no es cr√≠tico)
- Implementar timeout por tier (5s max)
- Parallel requests con Promise.race

### 3. Degradaci√≥n de UX
**S√≠ntoma:** Usuario espera mucho tiempo para ver resultado

**Causas:**
- Modal de evidence bloquea generaci√≥n
- Pre-search toma demasiado
- Report post-generation muy verbose

**Fix:**
- Hacer pre-search async (background)
- Mostrar preview de generaci√≥n mientras busca evidencia
- Report colapsable por defecto

---

## üí° Optimizaciones Futuras

### MentorChat: Evidence on Demand
```typescript
// Cuando el chat da n√∫meros/benchmarks:
if (responseContainsFactualClaim(aiResponse)) {
  showButton("üîç Respaldar con fuentes");
}

// Al hacer click:
// ‚Üí Invocar Evidence System solo para ese claim espec√≠fico
// ‚Üí Mostrar mini-report inline en el chat
```

**Benefit:** Evidence solo cuando importa, sin romper flujo conversacional

### Smart Profile Detection
```typescript
// Auto-detectar profile basado en claim type:
if (claimType === 'financial_metric') ‚Üí financial profile
if (claimType === 'market_size') ‚Üí financial profile
if (claimType === 'skill_requirement') ‚Üí learning profile
```

**Benefit:** M√°s preciso que inferir por function name

### Incremental Evidence
```typescript
// No esperar a tener TODAS las evidencias
// Mostrar generaci√≥n + evidencias conforme llegan

1. Generaci√≥n ‚Üí Show immediately
2. Tier 1 results ‚Üí Append
3. Tier 2 results ‚Üí Append
4. Final report ‚Üí Complete
```

**Benefit:** Percepci√≥n de velocidad mucho mejor

---

## ‚úÖ Checklist Final

Antes de marcar Evidence System como "Production Ready":

- [ ] Performance tests completados (baseline, balanced, strict)
- [ ] Coverage tests por profile
- [ ] Retrieval strategy validada por tier
- [ ] Red flags identificados y mitigados
- [ ] Plan de optimizaciones futuras documentado
- [ ] Graceful degradation testeado (ciudades peque√±as, APIs down)
- [ ] User feedback sobre tiempo de espera recolectado
- [ ] Dashboards de analytics configurados (tiempo por tier, hit rate)

**Criterio de √©xito:**
- 90% de generaciones completan en <15s (balanced mode)
- 70% de strict mode encuentra >2 sources relevantes
- 0% de timeouts en hypothesis mode
- User satisfaction >4/5 en "√∫til vs molesto"
