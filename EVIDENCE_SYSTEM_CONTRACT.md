# ğŸ“œ EVIDENCE SYSTEM - PRODUCTION CONTRACT

Este documento define el **contrato formal** del Evidence System para considerarlo production-ready.

---

## ğŸ¯ Performance SLA (Service Level Agreement)

### LÃ­mites Absolutos por Feature Type

| Feature Type | Evidence Mode | p50 Target | p95 Max | Timeout Max | Status |
|--------------|---------------|-----------|---------|-------------|--------|
| **Tasks** | Hypothesis (default) | < 2s | < 3s | 3s | â³ Pending validation |
| **Tasks** | Balanced | < 3s | < 5s | 4s | â³ Pending validation |
| **CRM** | Balanced | < 5s | < 7s | 8s | â³ Pending validation |
| **Learning** | Balanced | < 8s | < 12s | 15s | â³ Pending validation |
| **Team** | Balanced | < 6s | < 10s | 12s | â³ Pending validation |
| **GeoIntelligence** | Balanced | < 7s | < 12s | 15s | â³ Pending validation |
| **Financial** | Strict | < 15s | < 30s | 35s | â³ Pending validation |
| **CRM** | Strict | < 10s | < 20s | 25s | â³ Pending validation |
| **Learning** | Strict | < 12s | < 25s | 30s | â³ Pending validation |

### Timeout Handling Contract

**Problema:** En producciÃ³n habrÃ¡ APIs lentas, rate limits, spikes.

**SoluciÃ³n:** Timeouts estrictos + degradaciÃ³n graceful.

**Comportamiento esperado al expirar timeout:**

```typescript
// Timeout expira durante retrieval
{
  success: true,  // â† Sistema sigue funcionando
  evidence_status: 'partial',  // o 'no_evidence' si no rescatÃ³ nada
  sources_found: N,  // solo las que alcanzÃ³ antes de timeout
  timeout_occurred: true,
  timeout_details: {
    tier: 'official_apis',
    elapsed_ms: 8500,
    limit_ms: 8000
  },
  data: { /* output funcional */ }
}
```

**Reglas de timeout:**

1. **Timeout por tier:** Cada tier tiene timeout individual (2-5s dependiendo de complejidad)
2. **Early exit:** Si un tier timeout, continuar con el siguiente
3. **Never fail:** Timeout NO causa `success: false`
4. **Degradation:** `evidence_status` baja de `verified` â†’ `partial` â†’ `no_evidence`

**Acceptance criteria:**
- âœ… **PASS:** Cumple p50 + p95 + timeout handling testeado
- âš ï¸ **WARNING:** Cumple p50 pero falla p95 â†’ Hay outliers, investigar causas
- âŒ **FAIL:** Falla p50 O timeout causa error â†’ **BLOCKER**

**Importante:**
- Medimos percentiles (p50/p95), NO promedios
- Timeout debe probarse simulando APIs lentas (no solo happy path)

---

## ğŸ›¡ï¸ Graceful Degradation Contract

### Principio Fundamental
**El Evidence System NUNCA debe romper la funcionalidad core.**

### SeparaciÃ³n CrÃ­tica: System Health vs Evidence Quality

**IMPORTANTE:** Separar dos conceptos que NO deben mezclarse:

1. **`success`** (boolean) â†’ Salud del **sistema**
   - `true`: El sistema funciona (puede generar output)
   - `false`: Error de sistema (auth failure, storage error, parsing corrupto)

2. **`evidence_status`** (string) â†’ Calidad de la **evidencia**
   - `'verified'`: Evidencias suficientes y relevantes
   - `'partial'`: Algunas evidencias, pero menos de lo ideal
   - `'no_evidence'`: Sin evidencias (hypothesis mode)
   - `'error'`: Error al buscar evidencias (pero output generado)

**Regla de oro:**
```typescript
// âœ… CORRECTO: Falta evidencia pero sistema funciona
{ success: true, evidence_status: 'no_evidence' }

// âŒ INCORRECTO: Confundir falta de evidencia con error de sistema
{ success: false, error: 'No sources found' }  // â† NUNCA hacer esto
```

**ExcepciÃ³n para `success: false`:**

Solo usar cuando hay **error de sistema**, no por falta de evidencia:

| Scenario | success | evidence_status | JustificaciÃ³n |
|----------|---------|-----------------|---------------|
| No encontrÃ³ sources | `true` | `'no_evidence'` | Sistema funciona, solo falta evidencia |
| API timeout | `true` | `'partial'` o `'error'` | Sistema funciona, retrieval fallÃ³ |
| Auth failure (Supabase) | `false` | N/A | Sistema roto, no puede funcionar |
| Storage error (DB down) | `false` | N/A | Sistema roto, no puede guardar |
| Parsing corrupto (malformed data) | `false` | N/A | Sistema roto, data invÃ¡lida |
| User sin permisos | `false` | N/A | Sistema roto, unauthorized |

**Por quÃ© importa:**

Si mezclas errores de sistema con falta de evidencia:
- âŒ Logs se llenan de "errores" que no son bugs
- âŒ Alertas se disparan por falta de sources (falsos positivos)
- âŒ Debugging se vuelve imposible (no sabes si es bug o dato faltante)

### DefiniciÃ³n Precisa de Coverage (ANTI-GAMING)

**Problema:** `coverage_percentage` puede inflarse definiendo pocos claims.

**SoluciÃ³n:** DefiniciÃ³n formal que no puede manipularse.

#### FÃ³rmula Oficial
```typescript
coverage_percentage = (supported_claims / evidence_required_claims) * 100

where:
  supported_claims = claims with evidence_level >= 'medium'
  evidence_required_claims = claims where requires_evidence === true
```

#### Reglas de Conteo

**1. Solo cuentan claims que REQUIEREN evidencia:**
```typescript
// âœ… Cuenta para coverage
{ claim: "Salarios de developers en Madrid â‚¬45-60k", requires_evidence: true }

// âŒ NO cuenta para coverage (es creativo/interno)
{ claim: "Tareas priorizadas por impacto", requires_evidence: false }
```

**2. Niveles de evidencia:**
```typescript
evidence_level = 'strong'   â†’ cuenta como 1.0
evidence_level = 'medium'   â†’ cuenta como 1.0
evidence_level = 'weak'     â†’ cuenta como 0.5  // â† DECIDIDO: 0.5, no 0
evidence_level = 'none'     â†’ cuenta como 0.0
```

**Rationale para weak = 0.5:**
- Penaliza claims dÃ©bilmente soportados
- Pero da crÃ©dito parcial (mejor que nada)
- Incentiva buscar sources mÃ¡s fuertes

#### Ejemplo de CÃ¡lculo (No Manipulable)

**Escenario A: Gaming attempt (muchos claims sin requires_evidence)**
```typescript
claims = [
  { text: "Tasks basadas en proyecto", requires_evidence: false },  // â† NO cuenta
  { text: "Tasks basadas en proyecto", requires_evidence: false },  // â† NO cuenta
  { text: "Tasks basadas en proyecto", requires_evidence: false },  // â† NO cuenta
  { text: "Benchmark industria: 2-3 dÃ­as", requires_evidence: true, level: 'medium' }  // â† Cuenta
]

// Coverage = 1 / 1 = 100% (solo 1 claim requiere evidencia)
// âš ï¸ Parece alto, pero es correcto (el Ãºnico claim factual estÃ¡ soportado)
```

**Escenario B: LegÃ­timo (varios claims factuales)**
```typescript
claims = [
  { text: "Salario dev Madrid â‚¬45-60k", requires_evidence: true, level: 'strong' },  // 1.0
  { text: "Cost of living index 72", requires_evidence: true, level: 'medium' },     // 1.0
  { text: "PoblaciÃ³n Madrid 3.3M", requires_evidence: true, level: 'weak' },         // 0.5
  { text: "Competidores encontrados: 5", requires_evidence: true, level: 'none' }    // 0.0
]

// Coverage = (1.0 + 1.0 + 0.5 + 0.0) / 4 = 62.5%
```

#### ValidaciÃ³n Anti-Gaming

Para evitar manipulaciÃ³n, **auditar periÃ³dicamente**:

```typescript
// Red flag: Ratio de claims sin requires_evidence muy alto
const creative_ratio = claims.filter(c => !c.requires_evidence).length / claims.length;

if (creative_ratio > 0.80) {
  // âš ï¸ Warning: Puede estar evitando claims factuales para inflar coverage
  logWarning('High creative_ratio detected', { feature, creative_ratio });
}
```

**Target healthy ratio:**
- âœ… creative_ratio < 0.70 â†’ Balance normal
- âš ï¸ creative_ratio 0.70-0.85 â†’ Revisar si claims estÃ¡n bien clasificados
- âŒ creative_ratio > 0.85 â†’ Posible gaming, auditar manualmente

---

### Contrato por Escenario

#### 1. Fuentes Disponibles (Happy Path)
```typescript
{
  success: true,
  evidence_status: 'verified',
  sources_found: 5,
  coverage_percentage: 80,
  data: { /* output completo */ }
}
```
**UI:** Badge "âœ“ Verificado" + link a evidence report

#### 2. Fuentes Parciales
```typescript
{
  success: true,
  evidence_status: 'partial',
  sources_found: 1,  // < minSourcesOverall
  coverage_percentage: 30,
  data: { /* output completo */ }
}
```
**UI:** Badge "âš ï¸ Datos limitados" + botÃ³n "Reintentar bÃºsqueda"

#### 3. Sin Fuentes (Hypothesis Mode)
```typescript
{
  success: true,
  evidence_status: 'no_evidence',
  sources_found: 0,
  coverage_percentage: 0,
  data: { /* output completo (qualitative) */ }
}
```
**UI:** Badge "ğŸ’¡ HipÃ³tesis" + botÃ³n "ğŸ” Buscar mÃ¡s evidencia"

**CrÃ­tico:** En TODOS los casos, `success: true` y `data` presente.

#### 4. Error en Retrieval (APIs down, timeout)
```typescript
{
  success: true,  // â† No rompe
  evidence_status: 'error',
  sources_found: 0,
  error_details: "Timeout en official_apis",
  data: { /* output sin evidencia externa */ }
}
```
**UI:** Badge "âš ï¸ Error en bÃºsqueda" + botÃ³n "Reintentar" + output funcional

**NUNCA:**
- âŒ Devolver `success: false` si la generaciÃ³n core funcionÃ³
- âŒ Devolver `data: null`
- âŒ Lanzar error que mate la UI
- âŒ Bloquear al usuario sin output

---

## âš™ï¸ Claim-Based Retrieval (Tasks Profile)

### Problema
Tasks son decisiones **internas**. Buscar evidencia externa por defecto es waste.

### SoluciÃ³n: ActivaciÃ³n Condicional (HEURÃSTICA + FALLBACK)

**IMPORTANTE:** La detecciÃ³n automÃ¡tica es una **heurÃ­stica**, no una regla perfecta.

#### Mecanismo Multi-Layer

**Layer 1: Input Heuristic (rÃ¡pido, 90% accuracy)**
```typescript
// Detectar keywords en user input
const needsExternalData = /comparado|benchmark|industria|promedio|estÃ¡ndar|competencia|segÃºn mercado|vs competidores/.test(userInput);

if (needsExternalData) {
  activateClaim({ requires_evidence: true });
}
```

**Layer 2: Output Analysis (post-generation, 100% accuracy)**
```typescript
// DespuÃ©s de generar, analizar si output incluye nÃºmeros externos
const outputHasQuantitatives = /\d+%|\d+\s*(horas|dÃ­as|semanas)|promedio de \d+/.test(output);
const outputHasExternalClaims = /segÃºn industria|comparado con|benchmark de/.test(output);

if ((outputHasQuantitatives || outputHasExternalClaims) && sources_found === 0) {
  // Marcar claims como "unsupported"
  addWarning({
    type: 'unsupported_quantitative',
    message: 'Output incluye datos cuantitativos sin evidencia externa',
    claims: extractQuantitativeClaims(output)
  });
}
```

**Layer 3: Strict Mode Override**
```typescript
// User cambia a strict en modal
if (evidenceMode === 'strict') {
  activateClaim({ requires_evidence: true });  // Forzado
}
```

#### Fallback Strategy

Si la heurÃ­stica falla (no detecta en input pero output lo necesita):

1. **En balanced mode:**
   - Generar output sin retrieval
   - Post-anÃ¡lisis detecta quantitatives no soportados
   - Mostrar warning en UI: "âš ï¸ Datos estimados sin benchmark externo"
   - BotÃ³n: "ğŸ” Buscar evidencias para estos datos"

2. **En strict mode:**
   - Siempre hacer retrieval (no confiar en heurÃ­stica)

#### Target Metrics

- **90%** de tasks normales â†’ retrieval = 0
- **< 5%** false positives (retrieval activado innecesariamente)
- **< 5%** false negatives (output cuantitativo sin evidencia)

#### Test Cases

| User Input | Output Type | Retrieval Expected | Rationale |
|------------|-------------|-------------------|-----------|
| "Genera tareas para esta semana" | Interno | âŒ NO | DecisiÃ³n interna |
| "Genera tareas con tiempo promedio de industria" | Cuantitativo externo | âœ… YES | Heuristic detecta "promedio de industria" |
| "Genera tareas comparado con competencia" | Cuantitativo externo | âœ… YES | Heuristic detecta "comparado" |
| "Genera tareas optimizadas" | Interno | âŒ NO (pero check output) | Ambiguo, confiar en post-analysis |

**Importante:** Esto es MVP. En futuro, usar LLM para clasificar intent mÃ¡s precisamente.

---

## ğŸ“Š Retrieval Waste Metric (AMPLIADO)

### DefiniciÃ³n
**Waste:** Cuando buscas evidencias pero luego no las usas.

Hay **dos tipos de waste:**

1. **Coverage Waste:** Buscaste sources pero coverage es bajo
2. **Citation Waste:** Encontraste sources pero el modelo no las citÃ³

### FÃ³rmulas

#### 1. Coverage Waste (original)
```typescript
coverage_waste = (sources_found > 0 && coverage_percentage < threshold) ? 1 : 0
coverage_waste_rate = sum(coverage_waste) / total_generations
```

**Thresholds:**
- Balanced: coverage >= 30%
- Strict: coverage >= 50%

#### 2. Citation Utilization (NUEVO)
```typescript
citation_utilization = cited_sources / retrieved_sources

// Example:
// Retrieved 10 sources, model cited 2 â†’ utilization = 0.20 (20%)
```

**Target:**
- âœ… utilization >= 0.50 â†’ Retrieval eficiente (mitad de sources se usan)
- âš ï¸ utilization 0.25-0.50 â†’ SubÃ³ptimo (muchas sources no se citan)
- âŒ utilization < 0.25 â†’ **PROBLEMA** - Prompts no citan O sources irrelevantes

#### 3. Retrieval Cost (NUEVO)
```typescript
avg_retrieval_cost_ms = total_retrieval_time_ms / total_generations
avg_retrieval_calls = total_api_calls / total_generations

// Track per tier:
cost_by_tier = {
  user_docs: { avg_ms: 120, avg_calls: 1 },
  official_apis: { avg_ms: 2400, avg_calls: 3 },
  internal_data: { avg_ms: 80, avg_calls: 1 },
  web_news: { avg_ms: 3200, avg_calls: 5 }
}
```

**Utilidad:** Identificar quÃ© tier es mÃ¡s caro para optimizar.

### Red Flags (AMPLIADO)

| sources_found | coverage_% | cited | utilization | Waste Type | Root Cause | Fix |
|---------------|------------|-------|-------------|------------|------------|-----|
| 10 | 0% | 0 | 0.00 | âœ…âœ… BOTH | Prompts no citan | Forzar citaciÃ³n en prompt |
| 8 | 15% | 1 | 0.12 | âœ…âœ… BOTH | Model ignora sources | Revisar prompt + sources relevancia |
| 5 | 60% | 2 | 0.40 | âš ï¸ Citation | Sources parcialmente Ãºtiles | Mejorar queryHints |
| 3 | 80% | 3 | 1.00 | âŒ NONE | Perfecto | - |
| 0 | 0% | 0 | N/A | âŒ NONE | Hypothesis mode | Expected |

### Target Metrics (CONSOLIDADO)

| Metric | Target | Warning | Blocker |
|--------|--------|---------|---------|
| **coverage_waste_rate** | < 20% | 20-40% | > 40% |
| **citation_utilization** | >= 0.50 | 0.25-0.50 | < 0.25 |
| **avg_retrieval_cost_ms** | < 2000ms | 2000-4000ms | > 4000ms |

### DiagnÃ³stico por SÃ­ntoma

**SÃ­ntoma 1:** coverage_waste alto + utilization bajo
- **Causa:** Sources irrelevantes O prompts no citan
- **Fix:** Ajustar queryHints/synonyms + mejorar prompt

**SÃ­ntoma 2:** coverage_waste bajo + utilization bajo
- **Causa:** Model encuentra sources pero solo cita pocas
- **Fix:** Prompt forzar "cita al menos N sources"

**SÃ­ntoma 3:** retrieval_cost alto + utilization bajo
- **Causa:** Buscando en tiers caros sin ROI
- **Fix:** Eliminar tiers costosos (ej: web_news) del tierOrder

**Importante:** Si waste_rate > 40% O utilization < 0.25, mejor **apagar retrieval** que malgastar recursos.

---

## âœ… Production Readiness Checklist

### Fase 1: MediciÃ³n (OBLIGATORIO)
- [ ] Benchmarks de performance ejecutados (20 runs por feature)
- [ ] Percentiles calculados (p50, p95) para cada modo
- [ ] ComparaciÃ³n vs SLA documentada
- [ ] Retrieval waste medido
- [ ] Graceful degradation testeado (ciudades pequeÃ±as, APIs down)

### Fase 2: ValidaciÃ³n (BLOCKER si falla)
- [ ] Todas las features cumplen p50 target
- [ ] p95 no supera max (outliers controlados)
- [ ] waste_rate < 40% en todos los profiles
- [ ] GeoIntelligence funciona con 5 ciudades test (2 grandes, 3 pequeÃ±as)
- [ ] Error scenarios no rompen UI (APIs down, timeout, sin datos)

### Fase 3: OptimizaciÃ³n (Si es necesario)
- [ ] Features que fallan p95 â†’ Reducir tierOrder
- [ ] Features con waste alto â†’ Ajustar queryHints
- [ ] Profiles con retrieval innecesario â†’ Cambiar a hypothesis default
- [ ] Timeouts implementados por tier (5s max cada uno)

### Fase 4: Monitoreo (Post-deploy)
- [ ] Dashboard de analytics configurado
- [ ] Alertas de p95 > threshold
- [ ] Alertas de waste_rate > 30%
- [ ] User feedback sobre "Ãºtil vs molesto" recolectado

---

## ğŸšª Production Readiness Gate (PASS/FAIL)

### Criterios Duros para Deploy

Este es el **acceptance gate** final. Debe cumplirse **TODO** para considerar production-ready.

---

### âœ… PASS CRITERIA (Todos deben cumplirse)

#### 1. Performance SLA
- [ ] **p50 dentro de target** para cada feature type
- [ ] **p95 dentro de max** para cada feature type
- [ ] **timeout handling testeado** (simulando APIs down) â†’ No rompe

**Test:** 20 runs por feature, calcular percentiles, comparar vs tabla SLA.

---

#### 2. Evidence Status Funcionando
- [ ] **4 escenarios testeados:**
  - `verified` (sources >= minSourcesOverall, coverage >= 50%)
  - `partial` (sources < minSourcesOverall O coverage 20-50%)
  - `no_evidence` (sources = 0)
  - `error` (API down/timeout)
- [ ] **NUNCA** `success: false` por falta de evidencia
- [ ] **SIEMPRE** output funcional (incluso en `no_evidence`)

**Test:** Ejecutar cada feature en 4 condiciones (mock APIs para forzar escenarios).

---

#### 3. Waste Metrics Bajo Control
- [ ] **coverage_waste_rate < 20%** (global)
- [ ] **citation_utilization > 0.25** (global)
- [ ] **avg_retrieval_cost_ms < 2000ms** (global)

**Test:** 50 generaciones en producciÃ³n (o staging), medir mÃ©tricas, validar umbrales.

---

#### 4. GeoIntelligence Reliability
- [ ] **5 ciudades testeadas:**
  - 2 grandes (Madrid, Barcelona) â†’ `verified` o `partial`
  - 3 pequeÃ±as (Cuenca, Teruel, Soria) â†’ `no_evidence` PERO output Ãºtil
- [ ] **success: true** en TODAS (incluso sin datos)
- [ ] **UI muestra badge correcto** segÃºn evidence_status

**Test:** Ejecutar GeoIntelligence con 5 ciudades, validar degradaciÃ³n.

---

#### 5. Claim-Based Retrieval (Tasks)
- [ ] **90% de tasks normales** â†’ retrieval = 0
- [ ] **< 5% false positives** (retrieval activado innecesariamente)
- [ ] **< 5% false negatives** (output cuantitativo sin evidencia)

**Test:** 100 generaciones tasks variadas, medir activaciÃ³n de retrieval.

---

#### 6. Timeout No Causa Bloqueos
- [ ] **API timeout testeado** en cada profile
- [ ] **evidence_status baja** (verified â†’ partial â†’ no_evidence)
- [ ] **NUNCA bloqueo total** (siempre devuelve output)

**Test:** Mock API con delay 20s, validar comportamiento.

---

### âŒ FAIL CRITERIA (Cualquiera bloquea deploy)

#### Blockers Absolutos

1. âŒ **Alguna feature falla p50 target** â†’ BLOCKER
2. âŒ **waste_rate > 40%** en cualquier profile â†’ BLOCKER
3. âŒ **GeoIntelligence rompe** con ciudades pequeÃ±as (en vez de degradar) â†’ BLOCKER
4. âŒ **Hay casos donde `success: false`** por error en evidencia â†’ BLOCKER
5. âŒ **No se ejecutaron benchmarks** (sin datos = sin deploy) â†’ BLOCKER
6. âŒ **Timeout causa error fatal** en cualquier feature â†’ BLOCKER
7. âŒ **citation_utilization < 0.20** (global) â†’ BLOCKER (retrieval inÃºtil)

**AcciÃ³n:** NO deploy hasta fix.

---

### âš ï¸ WARNINGS (Tolerables con plan de mejora)

Permitido ir a producciÃ³n, pero con **plan de mejora documentado**:

1. âš ï¸ **p95 ligeramente sobre max** (< 20% exceso) â†’ Investigar outliers, plan 30 dÃ­as
2. âš ï¸ **waste_rate 20-30%** â†’ SubÃ³ptimo, plan de optimizaciÃ³n 60 dÃ­as
3. âš ï¸ **Algunas ciudades pequeÃ±as sin datos** â†’ OK si degrada gracefully
4. âš ï¸ **citation_utilization 0.20-0.25** â†’ SubÃ³ptimo, mejorar prompts

**AcciÃ³n:** Deploy permitido, pero tracking semanal + deadline para fix.

---

### ğŸ“Š Scorecard de Readiness

Completar antes de deploy:

| Criterion | Status | Evidence | Blocker? |
|-----------|--------|----------|----------|
| Performance SLA (p50) | â³ | Pending benchmarks | âŒ YES |
| Performance SLA (p95) | â³ | Pending benchmarks | âŒ YES |
| Timeout handling | â³ | Pending test | âŒ YES |
| Evidence status (4 scenarios) | â³ | Pending test | âŒ YES |
| Never success:false | â³ | Pending validation | âŒ YES |
| coverage_waste_rate < 20% | â³ | Pending measurement | âŒ YES |
| citation_utilization > 0.25 | â³ | Pending measurement | âŒ YES |
| GeoIntelligence (5 cities) | â³ | Pending test | âŒ YES |
| Claim-based retrieval (tasks) | â³ | Pending test | âŒ YES |

**DecisiÃ³n:**
- âœ… **PASS:** Todos los blockers resueltos â†’ **DEPLOY**
- âš ï¸ **PASS WITH WARNINGS:** Blockers OK, warnings con plan â†’ **DEPLOY + TRACKING**
- âŒ **FAIL:** AlgÃºn blocker pendiente â†’ **NO DEPLOY**

---

### ğŸ”„ Post-Deploy Monitoring (Primeros 7 dÃ­as)

DespuÃ©s de deploy, validar que el sistema se comporta en producciÃ³n real:

**Daily checks:**
- [ ] p95 sigue dentro de SLA
- [ ] waste_rate no aumentÃ³
- [ ] No hay error spikes por timeout
- [ ] User satisfaction tracking

**Criterio de rollback:**
- âŒ p95 excede SLA por >50% durante 2 dÃ­as consecutivos
- âŒ waste_rate > 40% durante 3 dÃ­as
- âŒ User complaints sobre lentitud > 10% de usuarios
- âŒ Errors relacionados con evidence > 1% de requests

---

## ğŸ“ˆ Success Metrics (Post-Deploy)

DespuÃ©s de 1 semana en producciÃ³n, evaluar:

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| p50 compliance rate | > 90% | - | â³ |
| p95 compliance rate | > 80% | - | â³ |
| waste_rate (global) | < 25% | - | â³ |
| User satisfaction | > 4/5 | - | â³ |
| Degradation success | 100% | - | â³ |
| Timeout rate | < 1% | - | â³ |

**Criterio de Ã©xito:**
- âœ… 5/6 mÃ©tricas en target â†’ Sistema estable
- âš ï¸ 3-4/6 â†’ Necesita ajustes
- âŒ < 3/6 â†’ Rollback, system no estÃ¡ listo

---

## ğŸ”„ IteraciÃ³n Continua

Este contrato NO es estÃ¡tico. Se revisa cada quarter:

1. **Q Review:** Â¿Los SLAs siguen siendo apropiados?
2. **Profile Tuning:** Â¿AlgÃºn profile necesita ajuste de tiers?
3. **New Features:** Â¿Nuevas features de IA necesitan Evidence System?
4. **Waste Analysis:** Â¿Podemos reducir waste_rate aÃºn mÃ¡s?

**Responsable:** Tech lead del Evidence System
**Frecuencia:** Cada 3 meses
**Output:** Updated contract + optimization roadmap
