# ğŸ”¬ EVIDENCE SYSTEM - OPERATION PHASE

## ğŸ¯ CAMBIO DE FASE RECONOCIDO

### âŒ Fase Anterior (DiseÃ±o)
```
DiseÃ±ar â†’ Optimizar â†’ MÃ¡s features â†’ MÃ¡s mÃ©tricas
```

**Problema:** Complejidad acumulada sin validaciÃ³n real.

### âœ… Fase Actual (OperaciÃ³n)
```
Instrumentar â†’ Medir â†’ Ajustar con datos reales
```

**Objetivo:** Comportamiento real bajo carga real.

---

## ğŸš« LO QUE NO HACEMOS

**STOP BUILDING:**
- âŒ MÃ¡s perfiles
- âŒ MÃ¡s mÃ©tricas en el contrato
- âŒ MÃ¡s heurÃ­sticas
- âŒ MÃ¡s optimizaciones especulativas

**RazÃ³n:** El riesgo NO es diseÃ±o. Es complejidad sin validaciÃ³n.

---

## âœ… PRIORIDADES ABSOLUTAS (EN ORDEN)

### 1ï¸âƒ£ InstrumentaciÃ³n Real (COMPLETADO âœ…)

**Archivo:** `src/lib/evidence/instrumentation.ts`

**QuÃ© hace:**
- Log estructurado de cada generaciÃ³n
- Persiste en DB (tabla `evidence_generation_metrics`)
- MÃ©tricas automÃ¡ticas: latency, retrieval, coverage, waste, user behavior
- CÃ¡lculo de percentiles (p50, p95)
- DetecciÃ³n de comportamiento de usuario

**MÃ©tricas crÃ­ticas:**
```typescript
{
  total_latency_ms,
  retrieval_time_ms,
  sources_found,
  sources_cited,
  citation_utilization,
  evidence_status,
  timeout_occurred,
  user_changed_mode,      // âš ï¸ CRÃTICO
  user_opened_report,     // âš ï¸ CRÃTICO
  user_regenerated,       // âš ï¸ CRÃTICO
}
```

**Schema SQL:** Incluido en el archivo.

**Action item:**
- [ ] Crear migration para `evidence_generation_metrics`
- [ ] Integrar `logEvidenceGeneration()` en cada feature
- [ ] Verificar que datos se persisten correctamente

---

### 2ï¸âƒ£ Timeout Handling (COMPLETADO âœ…)

**Archivo:** `src/lib/evidence/timeout-handler.ts`

**QuÃ© hace:**
- Hard cap global: 40s absoluto
- Soft caps por tier (3-10s dependiendo de tier)
- AbortController real (no Promise.race)
- Early exit si tier timeout
- Logging explÃ­cito de quÃ© tier fallÃ³
- NUNCA bloquear respuesta final

**Reglas:**
```typescript
// Si tier timeout:
- Continuar con siguiente tier
- evidence_status baja (verified â†’ partial â†’ no_evidence)
- NUNCA success: false

// Si global timeout (40s):
- Cortar retrieval inmediatamente
- Devolver sources encontrados hasta ese momento
- success: true, evidence_status: 'partial' o 'no_evidence'
```

**Action item:**
- [ ] Integrar `multiTierRetrieval()` en edge functions
- [ ] Testear con API mock lenta (20s delay)
- [ ] Validar que timeout NO rompe generaciÃ³n

---

### 3ï¸âƒ£ El Test Real (PENDIENTE â³)

**NO es p95.**

**ES comportamiento de usuario:**

#### Test 1: Â¿FricciÃ³n?
```sql
SELECT
  COUNT(*) FILTER (WHERE user_changed_mode = true) * 100.0 / COUNT(*) AS switched_rate
FROM evidence_generation_metrics
WHERE mode = 'hypothesis';
```

**Red flag:** `switched_rate > 30%` â†’ Usuarios huyen de evidence system.

**AcciÃ³n:** Revisar defaults. QuizÃ¡s `balanced` es too much.

---

#### Test 2: Â¿Defaults mal calibrados?
```sql
SELECT
  COUNT(*) FILTER (WHERE user_regenerated = true) * 100.0 / COUNT(*) AS regen_rate
FROM evidence_generation_metrics;
```

**Red flag:** `regen_rate > 20%` â†’ Evidencia insuficiente frecuentemente.

**AcciÃ³n:** Bajar minSourcesOverall o mejorar retrieval.

---

#### Test 3: Â¿Sobre-ingenierizaciÃ³n?
```sql
SELECT
  COUNT(*) FILTER (WHERE user_opened_report = false) * 100.0 / COUNT(*) AS ignored_rate
FROM evidence_generation_metrics;
```

**Red flag:** `ignored_rate > 70%` â†’ Nadie lee el report.

**AcciÃ³n:** Simplificar UI o eliminar report (waste de desarrollo).

---

## âš ï¸ Riesgo Real: UX PsicolÃ³gico

### Pregunta Mental

**Un founder quiere generar tareas rÃ¡pido.**

Â¿Siente que el sistema lo estÃ¡:
- âœ… Ayudando (dÃ¡ndole confianza con fuentes)?
- âŒ Auditando (forzÃ¡ndolo a esperar, ser riguroso)?

**El equilibrio es delicado.**

### SÃ­ntomas de FricciÃ³n

| SÃ­ntoma | Significado | AcciÃ³n |
|---------|-------------|--------|
| Usuarios cambian a hypothesis | Sistema too serious para tareas simples | Hacer hypothesis default para tasks |
| Usuarios cierran modal de evidencia | Pre-modal es molesto | Hacerlo opcional, mostrar solo en strict |
| Usuarios ignoran report | Sobre-ingenierizado | Simplificar o eliminar |
| GeneraciÃ³n > 5s percibida como lenta | Fatiga cognitiva | Reducir tiers o hacer retrieval async |

---

## ğŸ“Š Dashboard de MÃ©tricas (TO BUILD)

**Prioridad:** DespuÃ©s de tener datos (1 semana de producciÃ³n).

**Queries clave:**

### 1. Performance por Feature
```sql
SELECT
  feature,
  mode,
  PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_latency_ms) AS p50,
  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY total_latency_ms) AS p95
FROM evidence_generation_metrics
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY feature, mode
ORDER BY feature, mode;
```

### 2. Waste Rate
```sql
SELECT
  feature,
  COUNT(*) FILTER (WHERE waste_type != 'none') * 100.0 / COUNT(*) AS waste_rate
FROM evidence_generation_metrics
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY feature
HAVING waste_rate > 20
ORDER BY waste_rate DESC;
```

### 3. User Behavior (EL MÃS IMPORTANTE)
```sql
SELECT
  feature,
  COUNT(*) AS total,
  AVG(CASE WHEN user_changed_mode THEN 1 ELSE 0 END) AS switched_rate,
  AVG(CASE WHEN user_regenerated THEN 1 ELSE 0 END) AS regen_rate,
  AVG(CASE WHEN user_opened_report THEN 1 ELSE 0 END) AS report_open_rate
FROM evidence_generation_metrics
WHERE timestamp > NOW() - INTERVAL '7 days'
GROUP BY feature
ORDER BY total DESC;
```

---

## ğŸ”¥ Action Items (ESTA SEMANA)

### DÃ­a 1-2: IntegraciÃ³n de InstrumentaciÃ³n
- [ ] Crear migration para `evidence_generation_metrics`
- [ ] Integrar `logEvidenceGeneration()` en:
  - [ ] AILeadFinder
  - [ ] AITaskGenerator
  - [ ] LearningPathGenerator
  - [ ] OneOnOnePrep
  - [ ] GeoIntelligenceSelector
- [ ] Testear que datos se persisten

### DÃ­a 3-4: IntegraciÃ³n de Timeout Handling
- [ ] Integrar `multiTierRetrieval()` en edge functions principales:
  - [ ] `ai-lead-finder`
  - [ ] `generate-tasks-v2`
  - [ ] `generate-learning-path`
  - [ ] `geo-intelligence`
- [ ] Testear con API mock lenta
- [ ] Validar que timeout no rompe

### DÃ­a 5-7: MediciÃ³n Real
- [ ] Deploy a staging con instrumentaciÃ³n
- [ ] Ejecutar 50-100 generaciones variadas
- [ ] Analizar queries de mÃ©tricas
- [ ] Identificar red flags

---

## ğŸ¯ Criterios de Ã‰xito (1 Semana)

**NO es "sistema perfecto".**

**ES:**
1. âœ… Datos fluyendo a DB (instrumentaciÃ³n funciona)
2. âœ… Timeouts no rompen generaciÃ³n (degradaciÃ³n funciona)
3. âœ… Podemos responder:
   - Â¿CuÃ¡l es el p95 real de cada feature?
   - Â¿CuÃ¡ntos usuarios cambian a hypothesis?
   - Â¿CuÃ¡ntos ignoran el report?
   - Â¿DÃ³nde estÃ¡ el waste?

**Con esos datos â†’ Ajustar defaults.**

Sin esos datos â†’ Todo es especulaciÃ³n.

---

## ğŸ’¡ Insights Esperados (HipÃ³tesis)

DespuÃ©s de medir, probablemente encontraremos:

1. **Tasks:** p95 OK pero `switched_to_hypothesis` alto â†’ Hacer hypothesis default
2. **CRM:** `regen_rate` alto â†’ Bajar minSourcesOverall de 2 a 1
3. **Learning:** `report_ignored_rate` alto â†’ Simplificar UI del report
4. **Financial:** p95 cerca del lÃ­mite â†’ Reducir web_news del tierOrder

**Pero son solo hipÃ³tesis.**

**Los datos dirÃ¡n la verdad.**

---

## ğŸš¨ Red Flags para Rollback

Si en los primeros 7 dÃ­as vemos:

1. âŒ p95 > SLA en >50% de features â†’ System too slow
2. âŒ `switched_to_hypothesis` > 40% â†’ Friction too high
3. âŒ Error rate > 5% â†’ Bugs en timeout/degradation
4. âŒ User complaints sobre lentitud > 10% â†’ UX broken

**â†’ Rollback evidence system, volver a generaciÃ³n sin evidencia.**

**Mejor sin evidence que con bad UX.**

---

## ğŸ“ Lecciones del Cambio de Fase

### Lo que aprendimos:
1. âœ… DiseÃ±o sÃ³lido NO es suficiente
2. âœ… Complejidad sin validaciÃ³n es riesgo
3. âœ… MÃ©tricas de usuario > mÃ©tricas tÃ©cnicas
4. âœ… p95 no captura UX psicolÃ³gico

### Lo que hacemos ahora:
1. âœ… Instrumentar TODO
2. âœ… Medir comportamiento real
3. âœ… Ajustar con datos, no suposiciones
4. âœ… Priorizar UX sobre perfecciÃ³n tÃ©cnica

**Fase de operaciÃ³n â†’ Data-driven decisions.**
